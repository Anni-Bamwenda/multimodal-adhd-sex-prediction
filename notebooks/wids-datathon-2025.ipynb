{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: \n",
    "\n",
    "Neuropsychiatric disorders that occur in development, like anxiety, depression, autism, and attention deficit hyperactivity disorder, or ADHD, often differ in how and to what extent they affect males and females. ADHD occurs in about 11% of adolescents, with around 14% of boys and 8% of girls having a diagnosis. There is some evidence that girls with ADHD can often go undiagnosed, as they tend to have more inattentive symptoms which are harder to detect. Girls with ADHD who are undiagnosed will continue suffering with symptoms that burden their mental health and capacity to function.\n",
    "\n",
    "\n",
    "## Overview: \n",
    "\n",
    "In this year’s WiDS Datathon, participants will be tasked with building a model to predict both an individual’s sex and their ADHD diagnosis using functional brain imaging data of children and adolescents and their socio-demographic, emotions, and parenting information.\n",
    "\n",
    "## Challenge question:\n",
    "\n",
    "What brain activity patterns are associated with ADHD; are they different between males and females, and, if so, how?”\n",
    "\n",
    "## Challenge task:\n",
    "\n",
    "The task is to create a multi-outcome model to predict two separate target variables: 1) ADHD (1=yes or 0=no) and 2) female (1=yes or 0=no).\n",
    "\n",
    "## Why is this important? \n",
    "Tools of this nature can help identify individuals who may be at risk of ADHD, which can be difficult to diagnose particularly in females. Importantly, they help shed light on the parts of the brain relevant to ADHD in females and males, which in turn could lead to improvements in personalized medicine and therapies. Identifying ADHD early and designing therapies targeting specific brain mechanisms in a personalized way can greatly improve the mental health of affected individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install geomstats --target=/kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl --target=/kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra and statistics\n",
    "import pandas as pd # data processing\n",
    "import seaborn as sns # data visualization\n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import geomstats.backend as gs\n",
    "import openpyxl\n",
    "from sklearn.preprocessing import LabelEncoder # for feature engineering\n",
    "from sklearn.preprocessing import OneHotEncoder # for feature engineering\n",
    "from sklearn.preprocessing import StandardScaler # for data normalization\n",
    "from sklearn.preprocessing import MinMaxScaler # for data normalization\n",
    "from sklearn.preprocessing import RobustScaler # for data normalization\n",
    "from sklearn.metrics import f1_score # for model evaluation\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset\n",
    "from sklearn.impute import SimpleImputer # for feature engineering\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import geomstats.datasets.utils as data_utils\n",
    "from geomstats.geometry.skew_symmetric_matrices import SkewSymmetricMatrices\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data and Intial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the train datasets\n",
    "\n",
    "# train_mri = pd.read_csv('/kaggle/input/widsdatathon2025/TRAIN/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES.csv')\n",
    "train_mri = pd.read_csv('/kaggle/input/widsdatathon2025/TRAIN_NEW/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv')\n",
    "train_labels = pd.read_excel('/kaggle/input/widsdatathon2025/TRAIN_NEW/TRAINING_SOLUTIONS.xlsx')\n",
    "train_categorical = pd.read_excel('/kaggle/input/widsdatathon2025/TRAIN_NEW/TRAIN_CATEGORICAL_METADATA_new.xlsx')\n",
    "train_numerical = pd.read_excel('/kaggle/input/widsdatathon2025/TRAIN_NEW/TRAIN_QUANTITATIVE_METADATA_new.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the test datasets\n",
    "test_mri = pd.read_csv('/kaggle/input/widsdatathon2025/TEST/TEST_FUNCTIONAL_CONNECTOME_MATRICES.csv')\n",
    "test_categorical = pd.read_excel('/kaggle/input/widsdatathon2025/TEST/TEST_CATEGORICAL.xlsx')\n",
    "test_numerical = pd.read_excel('/kaggle/input/widsdatathon2025/TEST/TEST_QUANTITATIVE_METADATA.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train targets: \\n\", train_labels.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train MRI data: \\n\", train_mri.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train categorical data: \\n\", train_categorical.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train numerical features: \\n\", train_numerical.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapes of the train datasets:\")\n",
    "print(\"Shape of train_labels: \\n\", train_labels.shape)\n",
    "print(\"Shape of train_mri: \\n\", train_mri.shape)\n",
    "print(\"Shape of train_categorical: \\n\", train_categorical.shape)\n",
    "print(\"Shape of train_numerical: \\n\", train_numerical.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concise summary of the train datasets\n",
    "print(train_mri.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_categorical.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_numerical.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of our dataset\n",
    "print(train_mri.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_categorical.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_numerical.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the numerical features and get a better understanding of what we are working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numerical.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EHQ_EHQ_Total\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(x='EHQ_EHQ_Total', data=train_numerical, color='green')\n",
    "plt.title(\"Edinburgh Handedness Questionnaire\", fontsize=16)\n",
    "plt.xlabel(\"Laterality Index Score\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.show()\n",
    "# -100 = 10th left \n",
    "# −28 ≤ LI < 48 = middle \n",
    "# 100 = 10th right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDQ_SDQ_Conduct_Problems\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x='SDQ_SDQ_Conduct_Problems', data=train_numerical, palette = 'coolwarm')\n",
    "plt.title(\"Strength and Difficult Questionaire for Conduct Problems\", fontsize=16)\n",
    "plt.xlabel(\"Conduct Problems Scale\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDQ_SDQ_Emotional_Problems\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x='SDQ_SDQ_Emotional_Problems', data=train_numerical, palette = 'pastel')\n",
    "plt.title(\"Strength and Difficult Questionaire for Emotional Problems\", fontsize=16)\n",
    "plt.xlabel(\"Emotional Problems Scale\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDQ_SDQ_Externalizing and Internalizing\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='SDQ_SDQ_Externalizing', data=train_numerical, palette = 'Set2')\n",
    "plt.title(\"Externalizing Scores Distribution\", fontsize=16)\n",
    "plt.xlabel(\"Externalizing Score\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='SDQ_SDQ_Internalizing', data=train_numerical, palette = 'Set2')\n",
    "plt.title(\"Internalizing Scores Distribution\", fontsize=16)\n",
    "plt.xlabel(\"Internalizing score\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRI_Track_Age_at_Scan\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(x='MRI_Track_Age_at_Scan', kde=True, data=train_numerical, color='Maroon')\n",
    "plt.title(\"Distribution of Age during MRI Scan\", fontsize=16)\n",
    "plt.xlabel(\"Age\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADHD Distribution\n",
    "print(train_labels['ADHD_Outcome'].value_counts())\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x='ADHD_Outcome', data=train_labels, color='Skyblue')\n",
    "plt.title(\"ADHD Distribution\", fontsize=16)\n",
    "plt.xlabel(\"Outcome (1=Yes, 0=No)\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender Distribution\n",
    "print(train_labels['Sex_F'].value_counts())\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x='Sex_F', data=train_labels, color='Green')\n",
    "plt.title(\"Gender Distribution\", fontsize=16)\n",
    "plt.xlabel(\"Gender (0 = Male, 1 = Female)\", fontsize=12)\n",
    "plt.ylabel(\"Count\",fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of Emotional Problems with ADHD outcome\n",
    "train_numerical_copy = train_numerical.copy()\n",
    "train_numerical_copy['ADHD_Outcome'] = train_labels['ADHD_Outcome']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='ADHD_Outcome', y='SDQ_SDQ_Emotional_Problems', data=train_numerical_copy)\n",
    "plt.title('SDQ_SDQ_Emotional_Problems vs ADHD Outcome')\n",
    "plt.xlabel('ADHD Outcome')\n",
    "plt.ylabel('SDQ_SDQ_Emotional_Problems')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barratt_Barratt_P2_Occ - Barratt Simplified Measure of Social Status - Parent 1 Occupation\n",
    "train_categorical['Barratt_Barratt_P2_Occ'].value_counts()\n",
    "\n",
    "# 0=Homemaker, stay at home parent.\n",
    "# 5=Day laborer, janitor, house cleaner, farm worker, food counter,preparation worker, busboy.\n",
    "# 10=Garbage collector, short-order cook, cab driver, shoe sales, assembly line workers, masons, baggage porter.\n",
    "# 15=Pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x='Barratt_Barratt_P2_Occ', data=train_categorical[['Barratt_Barratt_P2_Occ']])\n",
    "plt.title(f\"Distribution of Barratt Social Status Measure - Parent 2 Occupation\", fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x='Barratt_Barratt_P1_Occ', data=train_categorical[['Barratt_Barratt_P1_Occ']])\n",
    "plt.title(f\"Distribution of Barratt Social Status Measure - Parent 1 Occupation\", fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare Parent level of education with ADHD Outcome\n",
    "\n",
    "sns.countplot(data=train_categorical, x='Barratt_Barratt_P1_Edu', hue=train_labels['ADHD_Outcome'])\n",
    "plt.title('ADHD Prevalence by Parent 1 Education')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.countplot(data=train_categorical, x='Barratt_Barratt_P2_Edu', hue=train_labels['ADHD_Outcome'])\n",
    "plt.title(\"ADHD Outcome Distribution by Parent 2 Education\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing color vision test and gender\n",
    "\n",
    "sns.countplot(data=train_numerical, x='ColorVision_CV_Score', hue=train_labels['Sex_F'])\n",
    "plt.title('Color Vision Score Distribution by Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographics - Race of Child vs ADHD Outcomes\n",
    "print(train_categorical['PreInt_Demos_Fam_Child_Race'].value_counts())\n",
    "\n",
    "# 0= White/Caucasian \n",
    "# 1= Black/African American \n",
    "# 2= Hispanic \n",
    "# 3= Asian \n",
    "# 4= Indian\n",
    "# 5= Native American India...\n",
    "\n",
    "\n",
    "sns.countplot(data=train_categorical, x='PreInt_Demos_Fam_Child_Race', hue=train_labels['ADHD_Outcome'])\n",
    "plt.title('ADHD Outcomes by Race of Child')\n",
    "plt.xlabel(\"Child Race\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "\n",
    "# Correlation of numerical features with train_labels\n",
    "\n",
    "# let's merge train_numerical with train_labels to create our dataset for correlation\n",
    "cat_corr_data = pd.merge(train_numerical, train_labels, on='participant_id')\n",
    "cat_corr_data.drop('participant_id', axis=1, inplace=True) # we won't need to check correlation with ids\n",
    "cat_corr_matrix = cat_corr_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed heat map\n",
    "# sns.heatmap(cat_corr_data, \n",
    "#             cmap='YlGnBu', # choosing a yellow-green-blue colormap\n",
    "#             annot=True, # Turning on annotations\n",
    "#             fmt=\"d\", # displaying annotations as integer\n",
    "#             linewidths=.5, # Add gridlines with width 0.5\n",
    "#             cbar=True, # Include color bar\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates and Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by checking and removing duplicates from our train_numerical and train_categorical datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates in our train data\n",
    "print(\"Len of train_numerical before: \", len(train_numerical))\n",
    "train_numerical.drop_duplicates() # removing duplicates if any\n",
    "print(\"Len of train_numerical after: \",len(train_numerical))\n",
    "\n",
    "\n",
    "print(\"Len of train_categorical before: \", len(train_categorical))\n",
    "train_categorical.drop_duplicates()#\n",
    "print(\"Len of train_categorical before: \", len(train_categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates in test data\n",
    "print(\"Len of test_numerical before: \", len(test_numerical))\n",
    "test_numerical.drop_duplicates() # removing duplicates if any\n",
    "print(\"Len of test_numerical after: \", len(test_numerical))\n",
    "\n",
    "print(\"Len of test_categorical before: \", len(test_categorical))\n",
    "test_categorical.drop_duplicates()\n",
    "print(\"Len of test_categorical after: \", len(test_categorical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that train and test datasets don't have any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(\"Missing values in train_numerical: \")\n",
    "train_numerical.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore why the column with missing values and find replacements (mean/median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numerical['MRI_Track_Age_at_Scan'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check how many kids were scanned at age 0. That may be due to a clerical error and we'll have to deal with it before replacing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numerical[train_numerical['MRI_Track_Age_at_Scan'] == 0]['MRI_Track_Age_at_Scan'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numerical[train_numerical['MRI_Track_Age_at_Scan'] == 0].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two datapoints with value of 0. Given the number is pretty low, we'll drop the rows from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the two rows with 'MRI_Track_Age_at_Scan' as 0\n",
    "train_numerical = train_numerical[train_numerical['MRI_Track_Age_at_Scan'] != 0]\n",
    "print(train_numerical[train_numerical['MRI_Track_Age_at_Scan'] == 0]['MRI_Track_Age_at_Scan'].value_counts())\n",
    "# From the output, the two rows are now dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the descriptive statistics of MRI column again.\n",
    "train_numerical['MRI_Track_Age_at_Scan'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now replace the missing values in 'MRI' with the mean\n",
    "train_numerical['MRI_Track_Age_at_Scan'].fillna(train_numerical['MRI_Track_Age_at_Scan'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check again for missing values in train numerical\n",
    "train_numerical.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values in train_categorical\n",
    "train_categorical.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's further investigate the 'PreInt_Demos_Fam_Child_Ethnicity' feature\n",
    "# 'PreInt_Demos_Fam_Child_Ethnicity' feature indicates the ethnicity of the Child\n",
    "# 0= Not Hispanic or Latino \n",
    "# 1= Hispanic or Latino \n",
    "# 2= Decline to specify \n",
    "# 3= Unknown\n",
    "print(\"Unique values for PreInt_Demos_Fam_Child_Ethnicity Feature: \")\n",
    "print(train_categorical['PreInt_Demos_Fam_Child_Ethnicity'].unique(), '\\n')\n",
    "print(\"Value counts for each unique value in PreInt_Demos_Fam_Child_Ethnicity:\")\n",
    "print(train_categorical['PreInt_Demos_Fam_Child_Ethnicity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since category 0 has the highest frequency, We'll replace the missing values with mode(0.0)\n",
    "train_categorical['PreInt_Demos_Fam_Child_Ethnicity'].fillna(train_categorical['PreInt_Demos_Fam_Child_Ethnicity'].mode().iloc[0], inplace=True)\n",
    "# We'll replace missing values in other categorical eatures with their mode as well\n",
    "train_categorical['PreInt_Demos_Fam_Child_Race'].fillna(train_categorical['PreInt_Demos_Fam_Child_Race'].mode().iloc[0], inplace = True)\n",
    "train_categorical['Barratt_Barratt_P1_Edu'].fillna(train_categorical['Barratt_Barratt_P1_Edu'].mode().iloc[0], inplace = True)\n",
    "train_categorical['Barratt_Barratt_P1_Occ'].fillna(train_categorical['Barratt_Barratt_P1_Occ'].mode().iloc[0], inplace = True)\n",
    "train_categorical['MRI_Track_Scan_Location'].fillna(train_categorical['MRI_Track_Scan_Location'].mode().iloc[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll drop Barratt_Barratt_P2_Edu and Barratt_Barratt_P2_Occ because they both have too many missing values\n",
    "drop_cols = ['Barratt_Barratt_P2_Edu', 'Barratt_Barratt_P2_Occ']\n",
    "train_categorical.drop(drop_cols, axis = 1, inplace = True)\n",
    "test_categorical.drop(drop_cols, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check to see if we removed all missing values from train_categorical\n",
    "train_categorical.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Outliers are anomalous or unusual values that significantly deviate from other observations.\n",
    " They can adversely impact the performance of our machine-learning models by introducing bias or skewness. \n",
    " Detecting outliers helps us maintain our dataset's integrity by ensuring all data falls within a reasonable range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some common methods to detect outliers are:\n",
    "1. **Z-score**\n",
    "\n",
    "    Z-score of a value is the distance between that value and the dataset's mean, expressed in terms of the standard deviation.\n",
    "    > z_score = (x - mean)/ standard deviation.\n",
    "    > \n",
    "    Values that have a **z-score greater than 3** are often considered to be outliers.\n",
    "\n",
    "2. **Interquartile Range** (IQR)\n",
    "\n",
    "    Interquartile range is the range between the first quartile (25th percentile) and third quartile (75th percentile).\n",
    "    Values that fall significantly below the first quartile (*lower bound*) or above the third quartile(*upper bound*) are often considered to be outliers.\n",
    "    > lower bound = Q1 - 1.5 * IQR\n",
    "\n",
    "    > upper bound = Q3 - 1.5 * IQR\n",
    "\n",
    "3. **Visualization** plots\n",
    "\n",
    "    Data visualization plots like countplots, scatterplots, and boxplots can be very helpful in visually detecting outliers from a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at features whose min values are significantly lower than their Q1 and features whose max values are significantly higher than Q3.\n",
    "\n",
    "Refer to our descriptive statistics section (*train_numerical.describe()*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with 'APQ_P_APQ_P_CP'\n",
    "Q1 = train_numerical['APQ_P_APQ_P_CP'].quantile(0.25)\n",
    "Q3 = train_numerical['APQ_P_APQ_P_CP'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5*IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "print(f'Lower bound for \"APQ_P_APQ_P_CP\" is: {lower_bound}')\n",
    "print(f'Upper bound for \"APQ_P_APQ_P_CP\" is: {upper_bound}')\n",
    "# Let's check how many values lie above the upper bound\n",
    "print(len(train_numerical[train_numerical['APQ_P_APQ_P_CP'] > upper_bound]))\n",
    "upper_bound_df = train_numerical[train_numerical['APQ_P_APQ_P_CP'] > upper_bound]\n",
    "print(upper_bound_df['APQ_P_APQ_P_CP'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the standard scaler to standardize our numerical columns\n",
    "scaler = StandardScaler() #initializing the scaler\n",
    "# dropping the participant_id column before standardizing the numerical columns\n",
    "train_numerical_scaled = scaler.fit_transform(train_numerical.drop(columns ='participant_id'))\n",
    "test_numerical_scaled = scaler.fit_transform(test_numerical.drop(columns = 'participant_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's combine numerical and categorical datasets into one dataframe\n",
    "train_combined = pd.merge(train_numerical, train_categorical,on =\"participant_id\", how =\"outer\").set_index(\"participant_id\")\n",
    "test_combined = pd.merge(test_numerical, test_categorical, on = \"participant_id\", how = \"outer\").set_index(\"participant_id\")\n",
    "# assert all(train_combined.index == train_labels.index), \"Label IDs don't match train IDs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing MRI Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape the connectome data into symmetric matrices\n",
    "\n",
    "We are given the upper half of the connectome matrices as vectors, which represent the functional connections between different brain regions. However, to analyze and process this data using Riemannian geometry-based methods, we need to reshape it into symmetric matrices.\n",
    "\n",
    "By reshaping the upper half vectors into symmetric matrices, we can reconstruct the full matrix, which is a more natural representation of the brain's functional connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ADHD solutions and sort the data by participant_id\n",
    "y_train_adhd = train_labels[['participant_id', 'ADHD_Outcome']].sort_values('participant_id')\n",
    "train_mri = train_mri.sort_values('participant_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the load_connectomes function\n",
    "def load_connectomes(train_mri, y_train_adhd, as_vectors=False):\n",
    "    \"\"\"\n",
    "    Load brain connectome data and ADHD labels, returning symmetric matrices with ones on the diagonal.\n",
    "    \"\"\"\n",
    "    \n",
    "    patient_id = gs.array(train_mri['participant_id'])\n",
    "    data = gs.array(train_mri.drop('participant_id', axis=1))\n",
    "    target = gs.array(y_train_adhd['ADHD_Outcome'])\n",
    "\n",
    "    if as_vectors:\n",
    "        return data, patient_id, target\n",
    "    mat = SkewSymmetricMatrices(200).matrix_representation(data)\n",
    "    mat = gs.eye(200) - gs.transpose(gs.tril(mat), (0, 2, 1))\n",
    "    mat = 1.0 / 2.0 * (mat + gs.transpose(mat, (0, 2, 1)))\n",
    "\n",
    "    return mat, patient_id, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the load_connectomes function\n",
    "data, patient_id, labels = load_connectomes(train_mri, y_train_adhd)\n",
    "\n",
    "# Print the results\n",
    "print(f\"There are {len(data)} connectomes: {sum(labels==0)} healthy patients and {sum(labels==1)} ADHD patients.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 200 x 200 matrices for each of the 1213 patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for SPD Manifold Membership\n",
    "\n",
    "Check if the connectome data lies on the Symmetric Positive Definite (SPD) manifold. We use the SPDMatrices class from the geomstats library to check for SPD property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geomstats.geometry.spd_matrices import SPDMatrices\n",
    "\n",
    "manifold = SPDMatrices(200, equip=False)\n",
    "print(gs.all(manifold.belongs(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of connectomes that do not lie on the SPD manifold\n",
    "\n",
    "count_false = np.sum(~(manifold.belongs(data)))\n",
    "print(\"Count of False:\", count_false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring SPD Property\n",
    "\n",
    "To ensure the data is Symmetric Positive Definite (SPD), we can add a small diagonal matrix to the original data. This approach modifies the data minimally while guaranteeing the SPD property. The small diagonal matrix is added to each 2D slice of the 3D matrix, but the correction is only non-zero for the slices that are not SPD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add a diagonal matrix to a 2D matrix\n",
    "def add_diagonal_correction(matrix):\n",
    "    eigenvalues = np.linalg.eigvals(matrix)\n",
    "    min_eigenvalue = np.min(eigenvalues)\n",
    "\n",
    "    if min_eigenvalue < 0:\n",
    "        correction = -min_eigenvalue + 1e-6\n",
    "        correction_matrix = correction * np.eye(matrix.shape[0])\n",
    "        return matrix + correction_matrix\n",
    "    else:\n",
    "        return matrix\n",
    "\n",
    "# Apply the correction to each 2D slice of the 3D matrix\n",
    "data_corrected = np.array([add_diagonal_correction(slice) for slice in data])\n",
    "\n",
    "print(\"Original Matrix shape:\", data.shape)\n",
    "print(\"Corrected Matrix shape:\", data_corrected.shape)\n",
    "\n",
    "print(gs.all(manifold.belongs(data_corrected)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting differences in original data and corrected data\n",
    "\n",
    "We expect the count of differences to be 12 X 200 = 2400, since we added a correction to 12 connectomes, each with 200 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_differences(array1, array2, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    This function compares two 3D arrays and returns the count of differences.\n",
    "    \"\"\"\n",
    "    if array1.shape != array2.shape:\n",
    "        raise ValueError(\"Arrays must be of the same shape\")\n",
    "    \n",
    "    differences = np.greater(np.abs(array1 - array2), tolerance)\n",
    "    count = np.sum(differences)\n",
    "    \n",
    "    return count\n",
    "\n",
    "print(count_differences(data, data_corrected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training mri data using RiemannianMinimumDistanceToMean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model for mri data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geomstats.learning.mdm import RiemannianMinimumDistanceToMean\n",
    "\n",
    "spd_manifold = SPDMatrices(n=200, equip=True)\n",
    "mdm = RiemannianMinimumDistanceToMean(space=spd_manifold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split mri data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data_corrected; y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print data statistics\n",
    "\n",
    "We examine the class distribution in the full dataset, as well as the train and test sets, to ensure that they are similar and representative of the overall data. This is crucial for training a reliable model, as a skewed class distribution can lead to biased results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dataset has {len(X)} connectomes.\")\n",
    "print(f\"The train set has {len(X_train)} connectomes and has size {X_train.shape}.\")\n",
    "print(f\"The test set has {len(X_test)} connectomes and has size {X_test.shape}.\")\n",
    "\n",
    "print(\"Full dataset class distribution:\")\n",
    "print(pd.Series(y).value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nTrain dataset class distribution:\")\n",
    "print(pd.Series(y_train).value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nTest dataset class distribution:\")\n",
    "print(pd.Series(y_test).value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdm.fit(X_train, y_train)\n",
    "print(\"Mdm score:\", mdm.score(X_test, y_test))\n",
    "\n",
    "y_pred = mdm.predict(X_test)\n",
    "print(\"F1 score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our F1 Score is a bit low, but now we have an idea of how to work with MRI data. We'll next use ensemble methods to train all our three data categories by combining separate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets (keeping MRI, numerical, and \n",
    "# categorical aligned)\n",
    "mri_train, mri_test, num_train, num_test, cat_train, cat_test, y_train, y_test = train_test_split(\n",
    "    train_mri, train_numerical, train_categorical, train_labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to load and preprocess a single MRI scan from a CSV ---\n",
    "def load_and_preprocess_mri(patient_id, csv_path):\n",
    "    try:\n",
    "        all_mri_data = pd.read_csv(csv_path)\n",
    "        patient_row = all_mri_data[all_mri_data['participant_id'] == patient_id]\n",
    "        if not patient_row.empty:\n",
    "            # Assuming the matrix data starts from the second column onwards\n",
    "            matrix_values = patient_row.iloc[:, 1:].values.flatten() # Flatten the row of matrix values\n",
    "            # Assuming the matrices are 36x36 (based on the filename pattern in TRAIN)\n",
    "            mri_scan = matrix_values.reshape(36, 36)\n",
    "            # Basic preprocessing: Normalize pixel values (if needed)\n",
    "            mri_scan = mri_scan / mri_scan.max() if mri_scan.max() > 0 else mri_scan\n",
    "            # Flatten the MRI scan for non-CNN models\n",
    "            flattened_mri = mri_scan.flatten()\n",
    "            return flattened_mri\n",
    "        else:\n",
    "            print(f\"MRI data not found for patient: {patient_id} in the CSV: {csv_path}\")\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"CSV file not found at: {csv_path}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Processing training MRI data ---\n",
    "train_mri_path = \"/kaggle/input/widsdatathon2025/TRAIN_NEW/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv\"\n",
    "X_train_mri = {}\n",
    "print(\"Loading and preprocessing training MRI data:\")\n",
    "for participant_id in tqdm(train_combined[\"participant_id\"]): # Assuming train_combined has 'participant_id'\n",
    "    mri_data = load_and_preprocess_mri(patient_id, train_mri_path)\n",
    "    if mri_data is not None:\n",
    "        X_train_mri[patient_id] = mri_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Processing test MRI data ---\n",
    "test_mri_path = \"/kaggle/input/widsdatathon2025/TEST/TEST_FUNCTIONAL_CONNECTOME_MATRICES.csv\"\n",
    "X_test_mri = {}\n",
    "print(\"Loading and preprocessing test MRI data:\")\n",
    "for patient_id in tqdm(test_ids):\n",
    "    mri_data = load_and_preprocess_mri(patient_id, test_mri_path)\n",
    "    if mri_data is not None:\n",
    "        X_test_mri[patient_id] = mri_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11498594,
     "sourceId": 90566,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
